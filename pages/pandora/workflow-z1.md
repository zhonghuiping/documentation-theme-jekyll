## 概念
工作流（Workflow）是一个数据接收、计算、导出工具，把业务流程映射到页面上，在这里您的数据业务得到可视化，方便您更直观地来进行大数据分析流程管理。它的操作方式类似于思维脑图，直接在组件上右键或者通过拖拽在组件之间进行连线即可。

![](http://p5bjfbphc.bkt.clouddn.com/zhaohang:workflow.png)

您可以通过访问Workflow主页的 [大数据工作流引擎](http://12.16.5.148:8080/pandora/dags) 进入工作流管理界面。


您可以根据需要创建一个或多个工作流，自行控制每一个工作流的`启动`和`停止`，方便您便捷地管理数据流。如果您给您的计算任务设置的是手动执行，那么您需要在工作流管理界面点击`执行`按钮，计算任务才会启动。通过多种功能自由组合，满足您的各种计算需求。

![](http://p5bjfbphc.bkt.clouddn.com/zhaohang:workflow.png)

点击`创建工作流`进入工作流编辑界面。

工作流提供2种组件：`数据源`和`导出`帮助您打通数据业务，一个完整的工作流至少要包含一个数据源组件和一个数据导出组件。后面会详述。在界面右上角您可以看到功能按钮：`更新&启动`、`更新`、`退出`。当您业务逻辑还没整理好的时候,但是想保存现有的操作，您可以点击`更新`来保存您当前所做的操作，这样做可以大大节省您的工作量。


工作流中字段的数据结构可以是以下几种类型：

|类型|解释|数据样例|
|:--|:--|:--|
|date|日期类型，默认展示格式为`RFC3339`，可自定义格式|`2017-01-01T15:00:25Z07:00`|
|string|字符串类型|"qiniu.com"|
|long|64位整数|1024|
|float|单精度64位浮点|322.00|
|boolean|布尔类型，值为`true`或`false`|false|



## 数据源

数据源是工作流的起始节点，它可以接收实时上传的数据或读取离线存储的数据，在工作流中，目前支持以下几种类型的数据源：

|名称|流式计算|批量计算|备注|
|:--|:--|:--|:--|
|消息队列|yes|no|只能作用于流式计算，实时接收用户上传的数据；每一条进入消息队列的数据，都会被存储2天时间，过期自动删除|
|对象存储|no|yes|只能作用于批量计算，可以一次性加载大量数据|
|CDN|no|yes|只能作用于批量计算，数据来源于七牛CDN服务|
|HDFS|no|yes|只能作用于批量计算，仅支持私有云，公有云不提供此服务|

!> 注意：创建好工作流之后，无论是否启动该工作流，消息队列节点都可以接收数据。

**消息队列节点相关参数填写**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|消息队列名称|
| 字段信息 |是|字段名称和字段类型|
| IP来源 |否|数据来源的IP信息|
|时间字段|否|数据接收的时间|
| 服务器内部反转译 |否|针对为了写入而被序列化产生的\\t和\\n进行反转译，恢复为\t和\n|

!> 注意：如果您的数据源新增了一些字段，可以使用`添加新字段`功能，更新消息队列。

**对象存储节点相关参数填写**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|对象存储数据源节点名称|
| 空间名称 |是|您要读取的文件所在的bucket名称|
| 文件类型 |是|您要读取文件的格式|
|文件前缀|是|您要读取的文件名称的前缀|

**CDN日志节点相关参数填写**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|CDN日志数据源节点名称|
| 域名 |是|您的CDN服务的域名|
| 文件过滤条件类型 |是|日志产生的时间范围的选择方式（固定时间/相对时间）|

当文件过滤条件类型选择“相对时间”时，过滤条件里可以引入`魔法变量`。魔法变量后文会详述。

![](https://pandora-kibana.qiniu.com/relative-time.png)




## 数据导出

将数据源或计算任务中的数据导出到指定的地址。

目前我们支持将数据导出到以下地址：

```
1. 指定一个日志分析服务的日志仓库；
2. 指定一个时序数据库的数据仓库下的序列；
3. 指定一个HTTP服务器地址；
4. 指定一个对象存储的Bucket；
5. 指定一个报表工作室的数据仓库下的数据表；
```
**导出到日志分析填写参数**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|导出节点名称|
| 仓库名称 |是|您要进行日志分析的仓库名称，可以选择已有仓库或者创建新仓库|
| 数据存储时限 |是|导出的数据存储在日志仓库的时间限制|
| 丢弃无效数据 |否|是否忽略无效数据|

**导出到时序数据仓库填写参数**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|导出节点名称|
| 数据库名称 |是|您要进行时序数据分析的数据仓库名称|
| 序列名称 |是|数据仓库的表名,数据将会被导入到这个表当中|
| 数据存储时限 |否|导出的数据存储在时序数据仓库的时间限制|
| 时间戳 |是|数据导出的时间，默认使用当前时间|
| 数据起始位置 |否|从最早还是最新数据开始导出|
| 丢弃无效数据 |否|是否忽略无效数据|

**导出到HTTP地址填写参数**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|导出节点名称|
| 服务器地址 |是|ip或域名，例如:https://pipeline.qiniu.com 或 https://127.0.0.1:7758|
| 请求资源路径 |是|具体地址，例如:/test/repos|
| 导出类型 |是|导出的文件格式|
| 数据起始位置 |是|从最早还是最新数据开始导出，默认从最新数据导出|

**导出到对象存储填写参数**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|导出节点名称|
| 空间名称 |是|导出的对象存储的bucket名称|
| 文件前缀 |是|导出的文件名称的前缀|
| 导出类型 |是|可以将文件导成四种格式：json、csv、text、parquet；其中json、csv和text可以选择是否将文件压缩，而parquet无需选择，默认自动压缩，压缩比大概为3-20倍|
| 文件压缩 |是|是否开启文件压缩功能|
| 最大文件保存天数 |是|数据导出在对象存储中的时限，以天为单位，超过这个时间范围的文件会被自动删除，当该字段为0或者为空时，则永久储存|
| 文件分割策略 |是|文件切割策略，可以按照文件大小切割：文件大小超过设置的值则进行切割；也可以按照时间间隔切割：文件导出时长超过设置的值则进行切割；也可以两者方式只要满足一种即进行切割|
| 数据起始位置 |是|从最早还是最新数据开始导出，默认从最新数据导出|

!> 关于文件前缀,默认值为空(生成文件名会自动加上时间戳格式为`yyyy-MM-dd-HH-mm-ss`),支持魔法变量。

前缀用法说明：

1.前缀使用魔法变量
  假如前缀的取值为kodo-parquet/date=$(year)-$(mon)-$(day)/hour=$(hour)/min=$(min)/$(sec),且生成某一文件时的北京标准时间为`2017-01-12 15:30:00`, 则前缀将被解析为kodo-parquet/date=2017-01-12/hour=15/min=30/00,其中的魔法变量$(year)、$(mon)、$(day)、$(hour)、$(min)、$(sec)分别对应文件生成时间`2017-01-12 15:30:00`的年、月、日、时、分、秒。
  
2.前缀使用默认值
  假如生成某一文件时的北京标准时间为`2017-01-12 15:30:00`, 则前缀将被解析为`2017-01-12-15-30-00`。
  
**导出到报表服务填写参数**

|参数|必填|说明|
|:---|:---|:---|
| 名称 |是|导出节点名称|
| 数据库名称 |是|导出到报表服务的数据库名称|
| 数据表名称 |是|导出的数据库的具体表名|
| 数据起始位置 |是|从最早还是最新数据开始导出，默认从最新数据导出|


## 运行日志监控

Workflow提供节点的运行日志监控，您可以选择数据节点（导出节点、流式计算节点、批量计算节点）来查看各节点的运行日志，帮您宏观监控数据的运行情况，及时发现问题。

![](https://pandora-kibana.qiniu.com/running_log1.png)



